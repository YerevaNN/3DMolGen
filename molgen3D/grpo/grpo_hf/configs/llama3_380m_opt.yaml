dataset:
  dataset_path: /nfs/ap/home/_menuab_/cartesianisomeric/grpo/train_data_full.jsonl
  smiles_mapping_path: /nfs/ap/mnt/sxtn2/chem/GEOM_data/smiles_mapping.json
device:
  device_type: h100
  num_gpus: 6
generation:
  do_sample: true
  max_completion_length: 1000
  num_return_sequences: 1
  repetition_penalty: 1.0
  temperature: 1.0
grpo:
  adam_beta1: 0.9
  adam_beta2: 0.95
  beta: 0.05
  checkpoint_base_dir: /nfs/h100/raid/chem/checkpoints/conf_grpo
  grad_acc_steps: 8
  learning_rate: 5.0e-05
  max_grad_norm: 1.0
  max_ground_truths: 64
  max_steps: 100
  num_epochs: 1
  num_generations: 16
  output_base_dir: ./grpo_outputs
  per_device_batch_size: 32
  reward_weight_match: 0.3
  reward_weight_rmsd: 1.0
  rmsd_const: 1.0
  scheduler: cosine
  seed: 1
  temperature: 1.0
  warmup_ratio: 0.15
  weight_decay: 0.1
model:
  checkpoint_path: /nfs/h100/raid/chem/checkpoints/hf/yerevann/Llama-3.2-380M_conformers/b4707c6bf8b54cba8febe9fc/step-8500/
  conf_tags:
  - '[CONFORMER]'
  - '[/CONFORMER]'
  dtype: bf16
  mol_tags:
  - '[SMILES]'
  - '[/SMILES]'
  pad_token: <|finetune_right_pad_id|>
  tokenizer_path: /auto/home/menuab/code/YNNtitan/torchtitan/tokenizers/Llama-3.2-chem-1B-v1
processing:
  eos_token_id: 128329
run:
  log_level: INFO
  name: 380m_1e_lr5e-5_32bs*16comp*6gpu*8ga_1e_full_beta0.05_alignFalse
trainer:
  attn_implementation: flash_attention_2
  log_on_each_node: false
  logging_steps: 1
  loss_type: grpo
  save_on_each_node: false
  save_safetensors: true
  save_steps: 500
  save_strategy: steps
  save_total_limit: 8
  torch_dtype: bfloat16
  use_liger_loss: true
