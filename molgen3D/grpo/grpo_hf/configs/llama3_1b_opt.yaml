model:
  checkpoint_path: /lustre/fsw/llmservice_nemo_reasoning/earakelyan/exp/checkpoints/hf-4.49.0/yerevann/Llama-3-1.3B/3fc60253e31447cc83f8f3da/step-20000
  tokenizer_path: /lustre/fsw/llmservice_nemo_reasoning/earakelyan/exp/tokenizers/Llama-3.2-chem-1B-v1
  mol_tags: ["[SMILES]", "[/SMILES]"]
  sim_tags: ["[SIMILAR]", "[/SIMILAR]"]
  pad_token: <|finetune_right_pad_id|>
  dtype: bf16 # bf16 / fp32

generation:
  max_new_tokens: 500
  temperature: 1.0
  do_sample: true
  repetition_penalty: 1.01
  num_return_sequences: 1

grpo:
  output_dir: /lustre/fsw/llmservice_nemo_reasoning/earakelyan/exp/grpo-hf
  learning_rate: 0.0001
  num_epochs: 1
  temperature: 1.0
  num_generations: 16
  batch_size: 16
  grad_acc_steps: 1
  scheduler: cosine
  # scheduler: constant_with_warmup
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_grad_norm: 0.1
  beta: 0.005
  seed: 1

# Dataset
dataset:
  task_names: ["dock.parp1"]
  mol_log_dirs: ["results/grpo-hf/llama3_380m_opt/2025-05-26-opt/exp-0"]
  num_samples: 300