[job]
description = "qwen3_06b_pre_4e_108effb"
dump_folder = "pretrain_runs"
print_config = true
custom_config_module = "molgen3D.training.pretraining.config.custom_job_config"

[molgen_run]
init_mode = "hf_pretrain" # scratch | hf_pretrain | resume
tokenizer_tag = "tokenizers:qwen3_0.6b_custom"
base_model_tag = "base_paths:qwen3_0.6b_base_model"
# resume_run_path_tag = "ckpts:qwen3_0.6b_previous_run"

[model]
name = "molgen_qwen3"
flavor = "0.6B"
hf_assets_path = "base_paths:qwen3_0.6b_base_model"

[experimental]
custom_import = "molgen3D.training.pretraining.torchtitan_model.qwen3_custom"

[training]
seq_len = 2048
local_batch_size = 12
global_batch_size = 144
steps = 20000
max_norm = 1.0
dtype = "bfloat16"
seed = 1234
dataset = "molgen_jsonl"
dataset_path = "data:conformers_train"

[validation]
enable = true
dataset = "molgen_jsonl"
dataset_path = "data:conformers_valid"
local_batch_size = 6
seq_len = 2048
freq = 500
steps = 200

[molgen_data]
train_path_key = "conformers_train"
tokenizer_key = "qwen3_0.6b_origin"
min_emb_len = 0
shuffle_lines = true
infinite = true
seed = 2025
num_workers = 8
pin_memory = true
drop_last = true
persistent_workers = true
prefetch_factor = 2
lookahead_limit = 100

[optimizer]
name = "AdamW"
lr = 2e-4
beta1 = 0.9
beta2 = 0.95
eps = 1e-8
weight_decay = 0.1

# [lr_scheduler]
# warmup_steps = 500
# decay_type = "cosine"

[wsds_scheduler]
enable = true
warmup_steps = 500
checkpoints = [20000, 40000, 60000, 80000]
lr_min = 0
decay_frac = 0.1

[checkpoint]
enable = true
interval = 20000
keep_latest_k = 4
folder = "checkpoint"
create_seed_checkpoint = false
initial_load_model_only = true
initial_load_in_hf = true
initial_load_path = "base_paths:qwen3_0.6b_base_model"
last_save_in_hf = true
async_mode = "async"

[metrics]
log_freq = 20
enable_wandb = true
save_for_all_ranks = false
save_tb_folder = "tb"

[parallelism]
data_parallel_replicate_degree = 1
data_parallel_shard_degree = -1
tensor_parallel_degree = 1
fsdp_reshard_after_forward = "default"
