dataloader:
  drop_last: false
  num_workers: 4
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 6
dataset:
  dataset_path: /nfs/ap/mnt/sxtn2/chem/GEOM_data/geom_processed/grpo/train_smiles.jsonl
  smiles_mapping_path: /nfs/ap/mnt/sxtn2/chem/GEOM_data/geom_processed/grpo/train_smiles_to_pickle.json
device:
  device_type: h100
  num_gpus: 8
generation:
  do_sample: true
  max_completion_length: 1500
  num_return_sequences: 1
  repetition_penalty: 1.0
  temperature: 1.0
grpo:
  adam_beta1: 0.9
  adam_beta2: 0.95
  beta: 0
  checkpoint_base_dir: /nfs/h100/raid/chem/checkpoints/conf_grpo
  delta: 0.75
  enable_pairwise_rmsd_logging: true
  enable_posebusters: true
  epsilon_high: 8e-4
  epsilon_low: 3e-4
  grad_acc_steps: 2
  importance_sampling_level: sequence
  lambda_match: 2.0
  lambda_qual: 2.0
  lambda_smcov: 4.0
  learning_rate: 3.0e-05
  log_distance_samples_per_group: 64
  max_grad_norm: 1.0
  max_ground_truths: 30
  max_steps: 8000
  num_epochs: 1
  num_generations: 16
  num_iterations: 1
  output_base_dir: ./grpo_outputs
  pairwise_rmsd_log_every: 50
  per_device_batch_size: 32
  posebusters:
    chunk_size: 16
    energy_num_threads: 1
    max_workers: 8
    mode: geometry
  r_floor: -1.0
  reward_strategy: v3
  rho: 0.6
  rmsd_const: 0.75
  scheduler: cosine
  seed: 1
  sigma: 0.25
  steps_per_generation: 8
  temperature: 1.0
  warmup_ratio: 0.03
  weight_decay: 0.0
model:
  checkpoint_path: /nfs/h100/raid/chem/checkpoints/yerevann/qwen3_06b/251210-1955-d79f-qwen3_06b_pre_4e_144effb/step-80000-hf/
  conf_tags:
  - '[CONFORMER]'
  - '[/CONFORMER]'
  dtype: bf16
  mol_tags:
  - '[SMILES]'
  - '[/SMILES]'
  pad_token: <|endoftext|>
  tokenizer_path: /auto/home/menuab/code/3DMolGen/src/molgen3D/training/tokenizers/Qwen3_tokenizer_custom
processing:
  eos_token_id: 151672
run:
  log_level: INFO
  name: qwen3_v3_match_qual_gspo_8*2*32comp_16steps
trainer:
  attn_implementation: flash_attention_2
  log_on_each_node: false
  logging_steps: 1
  loss_type: grpo
  save_on_each_node: false
  save_safetensors: true
  save_steps: 1000
  save_strategy: steps
  save_total_limit: 8
  torch_dtype: bfloat16
  use_liger_loss: false
