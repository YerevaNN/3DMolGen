dataloader:
  drop_last: false
  num_workers: 4
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 6
dataset:
  dataset_path: /nfs/ap/home/_menuab_/cartesianisomeric/grpo/train_data_full.jsonl
  smiles_mapping_path: /nfs/ap/mnt/sxtn2/chem/GEOM_data/smiles_mapping.json
device:
  device_type: a100
  num_gpus: 2
generation:
  do_sample: true
  max_completion_length: 1000
  num_return_sequences: 1
  repetition_penalty: 1.0
  temperature: 1.0
grpo:
  adam_beta1: 0.9
  adam_beta2: 0.95
  beta: 0
  checkpoint_base_dir: /nfs/h100/raid/chem/checkpoints/conf_grpo
  checkpoint_dir: /nfs/h100/raid/chem/checkpoints/conf_grpo/2025-09-30-16:14_380m_1e_lr5e-5_32bs*16comp*6gpu*8ga_1e_full_beta0.05_alignFalse
  grad_acc_steps: 8
  learning_rate: 2.0e-05
  max_grad_norm: 1.0
  max_ground_truths: 64
  max_steps: 4000
  num_epochs: 1
  num_generations: 16
  num_iterations: 4
  output_base_dir: ./grpo_outputs
  output_dir: /auto/home/menuab/code/3DMolGen/grpo_outputs/2025-09-30-16:14_380m_1e_lr5e-5_32bs*16comp*6gpu*8ga_1e_full_beta0.05_alignFalse
  per_device_batch_size: 8
  reward_weight_match: 0.15
  reward_weight_rmsd: 0.85
  rmsd_const: 0.75
  scheduler: cosine
  seed: 1
  temperature: 1.0
  warmup_ratio: 0.15
  weight_decay: 0.1
model:
  checkpoint_path: /nfs/h100/raid/chem/checkpoints/hf/yerevann/Llama-3.2-380M_conformers/e184d4d867804fbd8a8c45bf/step-30000/
  conf_tags:
  - '[CONFORMER]'
  - '[/CONFORMER]'
  dtype: bf16
  mol_tags:
  - '[SMILES]'
  - '[/SMILES]'
  pad_token: <|finetune_right_pad_id|>
  tokenizer_path: /auto/home/menuab/code/YNNtitan/torchtitan/tokenizers/Llama-3.2-chem-1B-v1
processing:
  eos_token_id: 128329
run:
  log_level: INFO
  name: 380m_4e_lr2e-5_32bs*16comp*4gpu*8ga_4ksteps_beta0_ne4
trainer:
  attn_implementation: flash_attention_2
  log_on_each_node: false
  logging_steps: 1
  loss_type: grpo
  save_on_each_node: false
  save_safetensors: true
  save_steps: 1000
  save_strategy: 'no'
  save_total_limit: 8
  torch_dtype: bfloat16
  use_liger_loss: true
