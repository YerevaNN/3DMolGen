[job]
dump_folder = "Llama-3.2-380M_conformers"

[model]
name = "llama3"
flavor = "debugmodel"
hf_assets_path = "src/molgen3D/training/tokenizers/Llama-3.2-chem-1B-v1"

[training]
steps = 10
seq_len = 2048
local_batch_size = 2
dtype = "bfloat16"
seed = 1337


dataset        = "custom"
dataset_module = "molgen3D.training.pretraining.dataprocessing.dataloader:build_dataloader"

# forwarded to your dataloader factory:
train_path      = "conformers_train"  # <-- update
tokenizer_path  = "llama3_chem_v1"  

[optimizer]
name = "AdamW"
lr = 3e-4
beta1 = 0.9
beta2 = 0.95
eps = 1e-8
weight_decay = 0.1

[lr_scheduler]
warmup_steps = 2000
decay_type = "cosine"

[checkpoint]
interval = 1000
keep_latest_k = 3

[compile]
enable = false

[metrics]
log_freq = 20
enable_wandb = true