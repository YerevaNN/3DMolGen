[job]
description = "testing qwen3_06b"
dump_folder = "qwen3_06b"
print_config = true
custom_config_module = "molgen3D.training.pretraining.config.custom_job_config"

[model]
name = "molgen_qwen3"
flavor = "0.6B"
hf_assets_path = "qwen3_0.6b_orig"

[training]
seq_len = 2048
local_batch_size = 12
global_batch_size = -1
steps = 5000
dtype = "float32"
mixed_precision_param = "bfloat16"
mixed_precision_reduce = "float32"
seed = 1234
dataset = "molgen_jsonl"
dataset_path = "conformers_train"

[optimizer]
name = "AdamW"
lr = 2e-4
beta1 = 0.9
beta2 = 0.95
eps = 1e-8
weight_decay = 0.1

[lr_scheduler]
warmup_steps = 400
decay_type = "cosine"
decay_ratio = 0.1

[checkpoint]
enable = true
interval = 5000
keep_latest_k = 2
last_save_in_hf = true

[metrics]
log_freq = 2
enable_wandb = true

[parallelism]
data_parallel_replicate_degree = 1
data_parallel_shard_degree = 4
fsdp_reshard_after_forward = "default"

[validation]
enable = true
dataset = "molgen_jsonl"
dataset_path = "conformers_valid"
local_batch_size = 12
freq = 250
steps = -1

[experimental]
custom_import = "molgen3D.training.torchtitan_model.qwen3_custom"

[molgen_data]
train_path_key = "conformers_train"
tokenizer_key = "qwen3_0.6b_orig"
min_emb_len = 16
shuffle_lines = true
infinite = true
seed = 2025
num_workers = 1
pin_memory = false
drop_last = true
persistent_workers = true
prefetch_factor = 2

